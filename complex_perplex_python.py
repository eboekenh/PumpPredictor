# %%

## 3. DetaylÄ± Veri Analizi (EDA - Exploratory Data Analysis)

### 3.1 Veri Setini Ä°nceleme

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# GÃ¶rselleÅŸtirme ayarlarÄ±
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Veri setlerini yÃ¼kle
train_values = pd.read_csv(r'C:\Users\birli\Documents\GitHub\Data_Circle_teamhamburg\data\training_set_values.csv')
train_labels = pd.read_csv(r'C:\Users\birli\Documents\GitHub\Data_Circle_teamhamburg\data\training_set_labels.csv')
test_values = pd.read_csv(r'C:\Users\birli\Documents\GitHub\Data_Circle_teamhamburg\data\test_set_values.csv')

# %%
# EÄŸitim verisini birleÅŸtir
# train_values: Ã–zellikler (features)
# train_labels: Hedef deÄŸiÅŸken (target)
train_df = train_values.merge(train_labels, on='id')

print(f"EÄŸitim seti boyutu: {train_df.shape}")
print(f"Test seti boyutu: {test_values.shape}")
print(f"\nÄ°lk 5 satÄ±r:")
print(train_df.head())

# %%

### 3.2 Veri YapÄ±sÄ± ve TÃ¼rleri

# Veri tipleri ve eksik deÄŸerleri incele
def analyze_data_structure(df, name='Dataset'):
    """
    Veri setinin yapÄ±sÄ±nÄ± detaylÄ± analiz eder
    
    Parameters:
    -----------
    df : DataFrame
        Analiz edilecek veri seti
    name : str
        Veri setinin adÄ± (Ã§Ä±ktÄ±da gÃ¶rÃ¼necek)
    """
    print(f"\n{'='*60}")
    print(f"{name} - Genel Bilgiler")
    print(f"{'='*60}\n")
    
    # Temel bilgiler
    print(f"SatÄ±r sayÄ±sÄ±: {df.shape[0]:,}")
    print(f"SÃ¼tun sayÄ±sÄ±: {df.shape[1]}")
    print(f"Toplam hÃ¼cre sayÄ±sÄ±: {df.shape[0] * df.shape[1]:,}")
    
    # Veri tipleri
    print(f"\n{'='*60}")
    print("Veri Tipleri DaÄŸÄ±lÄ±mÄ±:")
    print(f"{'='*60}")
    print(df.dtypes.value_counts())
    
    # Eksik deÄŸerler
    print(f"\n{'='*60}")
    print("Eksik DeÄŸer Analizi:")
    print(f"{'='*60}")
    missing = df.isnull().sum()
    missing_pct = 100 * missing / len(df)
    
    missing_df = pd.DataFrame({
        'SÃ¼tun': missing.index,
        'Eksik SayÄ±': missing.values,
        'Eksik YÃ¼zde (%)': missing_pct.values
    })
    
    # Sadece eksik deÄŸeri olan sÃ¼tunlarÄ± gÃ¶ster
    missing_df = missing_df[missing_df['Eksik SayÄ±'] > 0].sort_values(
        'Eksik SayÄ±', ascending=False
    )
    
    if len(missing_df) > 0:
        print(missing_df.to_string(index=False))
    else:
        print("Eksik deÄŸer bulunmamaktadÄ±r!")
    
    return missing_df

# Analizi Ã§alÄ±ÅŸtÄ±r
missing_analysis = analyze_data_structure(train_df, 'EÄŸitim Seti')

# %%

### 3.3 Hedef DeÄŸiÅŸken DaÄŸÄ±lÄ±mÄ±

def plot_target_distribution(df, target_col='status_group'):
    """
    Hedef deÄŸiÅŸkenin daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶rselleÅŸtirir
    
    Parameters:
    -----------
    df : DataFrame
        Veri seti
    target_col : str
        Hedef deÄŸiÅŸken sÃ¼tun adÄ±
    """
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ± - SayÄ±
    target_counts = df[target_col].value_counts()
    axes[0].bar(target_counts.index, target_counts.values, 
                color=['#2ecc71', '#f39c12', '#e74c3c'])
    axes[0].set_title('Pompa Durumu DaÄŸÄ±lÄ±mÄ± (SayÄ±)', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Durum', fontsize=12)
    axes[0].set_ylabel('Pompa SayÄ±sÄ±', fontsize=12)
    axes[0].tick_params(axis='x', rotation=45)
    
    # DeÄŸerleri barlarÄ±n Ã¼zerine yaz
    for i, v in enumerate(target_counts.values):
        axes[0].text(i, v + 500, f'{v:,}', ha='center', fontweight='bold')
    
    # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ± - YÃ¼zde
    target_pct = 100 * target_counts / len(df)
    axes[1].pie(target_pct.values, labels=target_pct.index, autopct='%1.1f%%',
                colors=['#2ecc71', '#f39c12', '#e74c3c'], startangle=90)
    axes[1].set_title('Pompa Durumu DaÄŸÄ±lÄ±mÄ± (%)', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('target_distribution.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Ä°statistiksel Ã¶zet
    print("\n" + "="*60)
    print("Hedef DeÄŸiÅŸken Ä°statistikleri:")
    print("="*60)
    summary_df = pd.DataFrame({
        'Durum': target_counts.index,
        'SayÄ±': target_counts.values,
        'YÃ¼zde (%)': target_pct.values
    })
    print(summary_df.to_string(index=False))
    
    # Dengesizlik oranÄ± (imbalance ratio)
    max_class = target_counts.max()
    min_class = target_counts.min()
    imbalance_ratio = max_class / min_class
    
    print(f"\nâš ï¸  SÄ±nÄ±f Dengesizlik OranÄ±: {imbalance_ratio:.2f}")
    if imbalance_ratio > 2:
        print("   â†’ Veri dengesiz! Ã–rnekleme teknikleri gerekebilir.")
    else:
        print("   â†’ Veri dengeli gÃ¶rÃ¼nÃ¼yor.")

# GÃ¶rselleÅŸtirmeyi Ã§alÄ±ÅŸtÄ±r
plot_target_distribution(train_df)

# %%

### 3.4 Kategorik DeÄŸiÅŸken Analizi

def analyze_categorical_features(df, target_col='status_group', top_n=5):
    """
    Kategorik deÄŸiÅŸkenleri analiz eder ve hedef deÄŸiÅŸkenle iliÅŸkisini gÃ¶sterir
    
    Parameters:
    -----------
    df : DataFrame
        Veri seti
    target_col : str
        Hedef deÄŸiÅŸken
    top_n : int
        Her kategoride gÃ¶sterilecek en yaygÄ±n deÄŸer sayÄ±sÄ±
    """
    # Kategorik sÃ¼tunlarÄ± bul
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    
    # Hedef deÄŸiÅŸkeni Ã§Ä±kar
    if target_col in categorical_cols:
        categorical_cols.remove(target_col)
    
    # 'id' sÃ¼tununu Ã§Ä±kar
    if 'id' in categorical_cols:
        categorical_cols.remove('id')
    
    print(f"\n{'='*60}")
    print(f"Toplam {len(categorical_cols)} kategorik deÄŸiÅŸken bulundu")
    print(f"{'='*60}\n")
    
    for col in categorical_cols[:10]:  # Ä°lk 10 kategorik deÄŸiÅŸkeni incele
        print(f"\n{'â”€'*60}")
        print(f"ğŸ“Š DeÄŸiÅŸken: {col}")
        print(f"{'â”€'*60}")
        
        # Benzersiz deÄŸer sayÄ±sÄ±
        n_unique = df[col].nunique()
        print(f"Benzersiz deÄŸer sayÄ±sÄ±: {n_unique}")
        
        # En yaygÄ±n deÄŸerler
        top_values = df[col].value_counts().head(top_n)
        print(f"\nEn yaygÄ±n {top_n} deÄŸer:")
        for val, count in top_values.items():
            pct = 100 * count / len(df)
            print(f"  â€¢ {val}: {count:,} ({pct:.2f}%)")
        
        # Hedef deÄŸiÅŸkenle Ã§apraz tablo
        if n_unique <= 10:  # Sadece az kategorili deÄŸiÅŸkenler iÃ§in
            print(f"\n{col} - {target_col} Ä°liÅŸkisi:")
            ct = pd.crosstab(df[col], df[target_col], normalize='index') * 100
            print(ct.round(2))

# Analizi Ã§alÄ±ÅŸtÄ±r
analyze_categorical_features(train_df)

# %%
### 3.5 SayÄ±sal DeÄŸiÅŸken Analizi

def analyze_numerical_features(df, target_col='status_group'):
    """
    SayÄ±sal deÄŸiÅŸkenleri analiz eder
    
    Parameters:
    -----------
    df : DataFrame
        Veri seti
    target_col : str
        Hedef deÄŸiÅŸken
    """
    # SayÄ±sal sÃ¼tunlarÄ± bul
    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    
    # 'id' sÃ¼tununu Ã§Ä±kar
    if 'id' in numerical_cols:
        numerical_cols.remove('id')
    
    print(f"\n{'='*60}")
    print(f"Toplam {len(numerical_cols)} sayÄ±sal deÄŸiÅŸken bulundu")
    print(f"{'='*60}\n")
    
    # Temel istatistikler
    stats_df = df[numerical_cols].describe().T
    stats_df['missing'] = df[numerical_cols].isnull().sum()
    stats_df['missing_pct'] = 100 * stats_df['missing'] / len(df)
    
    print("Temel Ä°statistikler:")
    print(stats_df.round(2))
    
    # GÃ¶rselleÅŸtirme: Box plot
    fig, axes = plt.subplots(
        nrows=(len(numerical_cols) + 2) // 3, 
        ncols=3, 
        figsize=(18, 4 * ((len(numerical_cols) + 2) // 3))
    )
    axes = axes.flatten()
    
    for idx, col in enumerate(numerical_cols):
        # AykÄ±rÄ± deÄŸerleri gÃ¶rmek iÃ§in box plot
        df.boxplot(column=col, by=target_col, ax=axes[idx])
        axes[idx].set_title(f'{col} DaÄŸÄ±lÄ±mÄ±')
        axes[idx].set_xlabel('Pompa Durumu')
        axes[idx].set_ylabel(col)
        plt.sca(axes[idx])
        plt.xticks(rotation=45)
    
    # KullanÄ±lmayan subplotlarÄ± gizle
    for idx in range(len(numerical_cols), len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    plt.savefig('numerical_features_boxplot.png', dpi=300, bbox_inches='tight')
    plt.show()

# Analizi Ã§alÄ±ÅŸtÄ±r
analyze_numerical_features(train_df)

# %%
### 3.6 CoÄŸrafi Analiz

def plot_geographical_distribution(df):
    """
    PompalarÄ±n coÄŸrafi daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶rselleÅŸtirir
    
    Parameters:
    -----------
    df : DataFrame
        Veri seti (latitude, longitude, status_group iÃ§ermeli)
    """
    # GeÃ§erli koordinatlarÄ± filtrele (0 olmayanlar)
    geo_df = df[(df['latitude'] != 0) & (df['longitude'] != 0)].copy()
    
    print(f"GeÃ§erli koordinat sayÄ±sÄ±: {len(geo_df):,} / {len(df):,}")
    
    # Durum kodlamasÄ± (renklendirme iÃ§in)
    status_colors = {
        'functional': '#2ecc71',
        'functional needs repair': '#f39c12',
        'non functional': '#e74c3c'
    }
    
    fig, ax = plt.subplots(figsize=(14, 10))
    
    for status, color in status_colors.items():
        mask = geo_df['status_group'] == status
        ax.scatter(
            geo_df[mask]['longitude'], 
            geo_df[mask]['latitude'],
            c=color, 
            label=status,
            alpha=0.5,
            s=10
        )
    
    ax.set_xlabel('Boylam (Longitude)', fontsize=12)
    ax.set_ylabel('Enlem (Latitude)', fontsize=12)
    ax.set_title('Tanzanya Su PompalarÄ± - CoÄŸrafi DaÄŸÄ±lÄ±m', 
                 fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('geographical_distribution.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # BÃ¶lgesel istatistikler
    print("\nBÃ¶lgesel DaÄŸÄ±lÄ±m (Region):")
    region_stats = pd.crosstab(
        df['region'], 
        df['status_group'], 
        normalize='index'
    ) * 100
    print(region_stats.round(2))

# GÃ¶rselleÅŸtirmeyi Ã§alÄ±ÅŸtÄ±r
plot_geographical_distribution(train_df)

# %%
## 4. Veri Ã–n Ä°ÅŸleme (Data Preprocessing)

### 4.1 Eksik DeÄŸer Ä°ÅŸleme

def handle_missing_values(df):
    """
    Eksik deÄŸerleri iÅŸler
    
    Parameters:
    -----------
    df : DataFrame
        Ä°ÅŸlenecek veri seti
    
    Returns:
    --------
    DataFrame
        Eksik deÄŸerleri iÅŸlenmiÅŸ veri seti
    """
    df_clean = df.copy()
    
    # 1. SayÄ±sal deÄŸiÅŸkenlerde eksik deÄŸerleri medyan ile doldur
    numerical_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns
    
    for col in numerical_cols:
        if df_clean[col].isnull().sum() > 0:
            median_val = df_clean[col].median()
            df_clean[col].fillna(median_val, inplace=True)
            print(f"âœ“ {col}: Eksik deÄŸerler {median_val} ile dolduruldu")
    
    # 2. Kategorik deÄŸiÅŸkenlerde eksik deÄŸerleri mod (en yaygÄ±n deÄŸer) ile doldur
    categorical_cols = df_clean.select_dtypes(include=['object']).columns
    
    for col in categorical_cols:
        if df_clean[col].isnull().sum() > 0:
            mode_val = df_clean[col].mode()[0]
            df_clean[col].fillna(mode_val, inplace=True)
            print(f"âœ“ {col}: Eksik deÄŸerler '{mode_val}' ile dolduruldu")
    
    # 3. Ã–zel durumlar
    # BazÄ± sayÄ±sal sÃ¼tunlarda 0 deÄŸeri 'eksik' anlamÄ±na gelebilir
    # Ã–rneÄŸin: population, construction_year, gps_height
    
    # construction_year = 0 ise (bilinmiyor), medyan ile doldur
    if 'construction_year' in df_clean.columns:
        mask = df_clean['construction_year'] == 0
        if mask.sum() > 0:
            valid_years = df_clean[df_clean['construction_year'] > 0]['construction_year']
            median_year = valid_years.median()
            df_clean.loc[mask, 'construction_year'] = median_year
            print(f"âœ“ construction_year: 0 deÄŸerleri {median_year} ile deÄŸiÅŸtirildi")
    
    # gps_height = 0 ise (deniz seviyesinde veya bilinmiyor)
    # Bu durumda ortalama ile doldurmak daha mantÄ±klÄ±
    if 'gps_height' in df_clean.columns:
        mask = df_clean['gps_height'] == 0
        if mask.sum() > 0:
            mean_height = df_clean[df_clean['gps_height'] != 0]['gps_height'].mean()
            df_clean.loc[mask, 'gps_height'] = mean_height
            print(f"âœ“ gps_height: 0 deÄŸerleri {mean_height:.2f} ile deÄŸiÅŸtirildi")
    
    # longitude/latitude = 0 ise (konum bilinmiyor), bÃ¶lge ortalamasÄ± ile doldur
    if 'latitude' in df_clean.columns and 'longitude' in df_clean.columns:
        mask = (df_clean['latitude'] == 0) | (df_clean['longitude'] == 0)
        if mask.sum() > 0:
            # BÃ¶lge bazÄ±nda ortalama koordinatlar
            if 'region' in df_clean.columns:
                for region in df_clean['region'].unique():
                    region_mask = (df_clean['region'] == region) & mask
                    if region_mask.sum() > 0:
                        region_coords = df_clean[
                            (df_clean['region'] == region) & 
                            (df_clean['latitude'] != 0)
                        ]
                        if len(region_coords) > 0:
                            mean_lat = region_coords['latitude'].mean()
                            mean_lon = region_coords['longitude'].mean()
                            df_clean.loc[region_mask, 'latitude'] = mean_lat
                            df_clean.loc[region_mask, 'longitude'] = mean_lon
            
            print(f"âœ“ latitude/longitude: 0 deÄŸerleri bÃ¶lge ortalamalarÄ± ile deÄŸiÅŸtirildi")
    
    print(f"\n{'='*60}")
    print("Eksik DeÄŸer Ä°ÅŸleme TamamlandÄ±!")
    print(f"{'='*60}")
    print(f"Kalan eksik deÄŸer sayÄ±sÄ±: {df_clean.isnull().sum().sum()}")
    
    return df_clean

# Fonksiyonu uygula
train_clean = handle_missing_values(train_df)
test_clean = handle_missing_values(test_values)

### 4.2 Kategorik DeÄŸiÅŸken Encoding

from sklearn.preprocessing import LabelEncoder

def encode_categorical_features(train_df, test_df, target_col='status_group'):
    """
    Kategorik deÄŸiÅŸkenleri sayÄ±sal deÄŸerlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
    
    Parameters:
    -----------
    train_df : DataFrame
        EÄŸitim veri seti
    test_df : DataFrame
        Test veri seti
    target_col : str
        Hedef deÄŸiÅŸken (encoding'e dahil edilmeyecek)
    
    Returns:
    --------
    tuple
        (train_encoded, test_encoded, label_encoders)
    """
    train_encoded = train_df.copy()
    test_encoded = test_df.copy()
    
    # Kategorik sÃ¼tunlarÄ± bul
    categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()
    
    # Hedef deÄŸiÅŸkeni ve id'yi Ã§Ä±kar
    if target_col in categorical_cols:
        categorical_cols.remove(target_col)
    if 'id' in categorical_cols:
        categorical_cols.remove('id')
    
    # Her kategorik deÄŸiÅŸken iÃ§in LabelEncoder
    label_encoders = {}
    
    for col in categorical_cols:
        print(f"Encoding: {col}...")
        
        # LabelEncoder oluÅŸtur
        le = LabelEncoder()
        
        # Train ve test'i birleÅŸtir (tÃ¼m kategorileri Ã¶ÄŸrenmek iÃ§in)
        combined = pd.concat([
            train_df[col].astype(str), 
            test_df[col].astype(str)
        ])
        
        # Fit et
        le.fit(combined)
        
        # Transform et
        train_encoded[col] = le.transform(train_df[col].astype(str))
        test_encoded[col] = le.transform(test_df[col].astype(str))
        
        # Encoder'Ä± sakla (gelecekte yeni verileri encode etmek iÃ§in)
        label_encoders[col] = le
        
        print(f"  âœ“ {col}: {len(le.classes_)} benzersiz kategori encode edildi")
    
    # Hedef deÄŸiÅŸkeni de encode et (sadece train iÃ§in)
    if target_col in train_encoded.columns:
        target_le = LabelEncoder()
        train_encoded[target_col] = target_le.fit_transform(train_df[target_col])
        label_encoders[target_col] = target_le
        
        print(f"\nâœ“ Hedef deÄŸiÅŸken ({target_col}) encode edildi:")
        for idx, label in enumerate(target_le.classes_):
            print(f"  {label} â†’ {idx}")
    
    print(f"\n{'='*60}")
    print("Kategorik Encoding TamamlandÄ±!")
    print(f"{'='*60}")
    
    return train_encoded, test_encoded, label_encoders

# Encoding'i uygula
train_encoded, test_encoded, encoders = encode_categorical_features(
    train_clean, 
    test_clean
)

### 4.3 Ã–zellik Ã–lÃ§eklendirme (Feature Scaling)

from sklearn.preprocessing import StandardScaler

def scale_features(train_df, test_df, target_col='status_group'):
    """
    SayÄ±sal Ã¶zellikleri standartlaÅŸtÄ±rÄ±r (0 ortalama, 1 standart sapma)
    
    Parameters:
    -----------
    train_df : DataFrame
        EÄŸitim veri seti
    test_df : DataFrame
        Test veri seti
    target_col : str
        Hedef deÄŸiÅŸken (Ã¶lÃ§eklendirmeye dahil edilmeyecek)
    
    Returns:
    --------
    tuple
        (train_scaled, test_scaled, scaler)
    """
    train_scaled = train_df.copy()
    test_scaled = test_df.copy()
    
    # SayÄ±sal sÃ¼tunlarÄ± bul
    numerical_cols = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    
    # id ve hedef deÄŸiÅŸkeni Ã§Ä±kar
    if 'id' in numerical_cols:
        numerical_cols.remove('id')
    if target_col in numerical_cols:
        numerical_cols.remove(target_col)
    
    # StandardScaler oluÅŸtur
    scaler = StandardScaler()
    
    # Train setine fit et
    scaler.fit(train_df[numerical_cols])
    
    # Hem train hem test'i transform et
    train_scaled[numerical_cols] = scaler.transform(train_df[numerical_cols])
    test_scaled[numerical_cols] = scaler.transform(test_df[numerical_cols])
    
    print(f"{'='*60}")
    print(f"Ã–lÃ§eklendirme TamamlandÄ±!")
    print(f"{'='*60}")
    print(f"Ã–lÃ§eklendirilen deÄŸiÅŸken sayÄ±sÄ±: {len(numerical_cols)}")
    print(f"\nÃ–lÃ§eklendirilen deÄŸiÅŸkenler:")
    for col in numerical_cols:
        original_mean = train_df[col].mean()
        scaled_mean = train_scaled[col].mean()
        print(f"  â€¢ {col}: {original_mean:.2f} â†’ {scaled_mean:.6f}")
    
    return train_scaled, test_scaled, scaler

# Ã–lÃ§eklendirmeyi uygula
train_scaled, test_scaled, scaler = scale_features(train_encoded, test_encoded)


## 5. Ã–zellik MÃ¼hendisliÄŸi (Feature Engineering)

### 5.1 Tarih BazlÄ± Ã–zellikler

def create_date_features(df):
    """
    Tarih sÃ¼tunlarÄ±ndan yeni Ã¶zellikler tÃ¼retir
    
    Parameters:
    -----------
    df : DataFrame
        Veri seti
    
    Returns:
    --------
    DataFrame
        Yeni Ã¶zellikler eklenmiÅŸ veri seti
    """
    df_new = df.copy()
    
    # Mevcut yÄ±l (veri seti 2013'te toplanmÄ±ÅŸ)
    current_year = 2013
    
    if 'construction_year' in df_new.columns:
        # PompanÄ±n yaÅŸÄ±
        df_new['pump_age'] = current_year - df_new['construction_year']
        
        # Negatif yaÅŸlarÄ± 0 yap (henÃ¼z inÅŸa edilmemiÅŸ)
        df_new.loc[df_new['pump_age'] < 0, 'pump_age'] = 0
        
        print(f"âœ“ 'pump_age' Ã¶zelliÄŸi oluÅŸturuldu")
        print(f"  Ortalama pompa yaÅŸÄ±: {df_new['pump_age'].mean():.2f} yÄ±l")
        
        # Pompa yaÅŸ kategorisi
        df_new['pump_age_category'] = pd.cut(
            df_new['pump_age'],
            bins=[0, 5, 10, 20, 100],
            labels=['Yeni (0-5)', 'GenÃ§ (5-10)', 'Orta (10-20)', 'Eski (20+)']
        )
        print(f"âœ“ 'pump_age_category' Ã¶zelliÄŸi oluÅŸturuldu")
    
    # date_recorded varsa (pompanÄ±n kaydedilme tarihi)
    if 'date_recorded' in df_new.columns:
        df_new['date_recorded'] = pd.to_datetime(df_new['date_recorded'])
        
        # Ay
        df_new['recorded_month'] = df_new['date_recorded'].dt.month
        print(f"âœ“ 'recorded_month' Ã¶zelliÄŸi oluÅŸturuldu")
        
        # Mevsim
        df_new['recorded_season'] = df_new['recorded_month'].apply(
            lambda x: 'KÄ±ÅŸ' if x in [12, 1, 2] else
                     'Ä°lkbahar' if x in [3, 4, 5] else
                     'Yaz' if x in [6, 7, 8] else 'Sonbahar'
        )
        print(f"âœ“ 'recorded_season' Ã¶zelliÄŸi oluÅŸturuldu")
        
        # YÄ±l iÃ§indeki gÃ¼n
        df_new['recorded_day_of_year'] = df_new['date_recorded'].dt.dayofyear
    
    print(f"\n{'='*60}")
    print("Tarih BazlÄ± Ã–zellikler OluÅŸturuldu!")
    print(f"{'='*60}")
    
    return df_new

# Tarih Ã¶zelliklerini oluÅŸtur
train_with_dates = create_date_features(train_scaled)
test_with_dates = create_date_features(test_scaled)

### 5.2 CoÄŸrafi Ã–zellikler

def create_geographical_features(df):
    """
    CoÄŸrafi koordinatlardan yeni Ã¶zellikler tÃ¼retir
    
    Parameters:
    -----------
    df : DataFrame
        Veri seti (latitude, longitude iÃ§ermeli)
    
    Returns:
    --------
    DataFrame
        Yeni coÄŸrafi Ã¶zellikler eklenmiÅŸ veri seti
    """
    df_new = df.copy()
    
    if 'latitude' in df_new.columns and 'longitude' in df_new.columns:
        # Tanzanya'nÄ±n merkezi (yaklaÅŸÄ±k)
        tanzania_center_lat = -6.369028
        tanzania_center_lon = 34.888822
        
        # Merkezden uzaklÄ±k (Haversine formÃ¼lÃ¼ ile)
        from math import radians, sin, cos, sqrt, atan2
        
        def haversine_distance(lat1, lon1, lat2, lon2):
            """
            Ä°ki GPS koordinatÄ± arasÄ±ndaki mesafeyi hesaplar (km)
            """
            R = 6371  # DÃ¼nya yarÄ±Ã§apÄ± (km)
            
            lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
            dlat = lat2 - lat1
            dlon = lon2 - lon1
            
            a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
            c = 2 * atan2(sqrt(a), sqrt(1-a))
            distance = R * c
            
            return distance
        
        # Her pompa iÃ§in merkeze uzaklÄ±ÄŸÄ± hesapla
        df_new['distance_from_center'] = df_new.apply(
            lambda row: haversine_distance(
                row['latitude'], row['longitude'],
                tanzania_center_lat, tanzania_center_lon
            ),
            axis=1
        )
        
        print(f"âœ“ 'distance_from_center' Ã¶zelliÄŸi oluÅŸturuldu")
        print(f"  Ortalama uzaklÄ±k: {df_new['distance_from_center'].mean():.2f} km")
        
        # YÃ¼kselti kategorisi (gps_height varsa)
        if 'gps_height' in df_new.columns:
            df_new['elevation_category'] = pd.cut(
                df_new['gps_height'],
                bins=[-100, 500, 1000, 1500, 3000],
                labels=['DÃ¼ÅŸÃ¼k', 'Orta', 'YÃ¼ksek', 'Ã‡ok YÃ¼ksek']
            )
            print(f"âœ“ 'elevation_category' Ã¶zelliÄŸi oluÅŸturuldu")
    
    print(f"\n{'='*60}")
    print("CoÄŸrafi Ã–zellikler OluÅŸturuldu!")
    print(f"{'='*60}")
    
    return df_new

# CoÄŸrafi Ã¶zellikleri oluÅŸtur
train_with_geo = create_geographical_features(train_with_dates)
test_with_geo = create_geographical_features(test_with_dates)


### 5.3 Toplama/Gruplama BazlÄ± Ã–zellikler

def create_aggregated_features(train_df, test_df, group_cols):
    """
    Gruplama bazlÄ± istatistiksel Ã¶zellikler oluÅŸturur
    
    Parameters:
    -----------
    train_df : DataFrame
        EÄŸitim veri seti
    test_df : DataFrame
        Test veri seti
    group_cols : list
        Gruplama yapÄ±lacak sÃ¼tunlar
    
    Returns:
    --------
    tuple
        (train_with_agg, test_with_agg)
    """
    train_new = train_df.copy()
    test_new = test_df.copy()
    
    # Her grup iÃ§in pompa sayÄ±sÄ± ve ortalama yaÅŸ
    for col in group_cols:
        if col in train_new.columns:
            print(f"\n{'â”€'*60}")
            print(f"Gruplama: {col}")
            print(f"{'â”€'*60}")
            
            # Pompa sayÄ±sÄ± (bu kategoride kaÃ§ pompa var?)
            group_counts = train_new[col].value_counts().to_dict()
            train_new[f'{col}_pump_count'] = train_new[col].map(group_counts)
            test_new[f'{col}_pump_count'] = test_new[col].map(group_counts)
            print(f"âœ“ '{col}_pump_count' oluÅŸturuldu")
            
            # Ortalama pompa yaÅŸÄ± (bu kategoride pompalar ortalama kaÃ§ yaÅŸÄ±nda?)
            if 'pump_age' in train_new.columns:
                age_mean = train_new.groupby(col)['pump_age'].mean().to_dict()
                train_new[f'{col}_avg_age'] = train_new[col].map(age_mean)
                test_new[f'{col}_avg_age'] = test_new[col].map(age_mean)
                print(f"âœ“ '{col}_avg_age' oluÅŸturuldu")
            
            # ArÄ±za oranÄ± (bu kategoride ne kadar pompa bozuk?)
            if 'status_group' in train_new.columns:
                # functional = 0, functional needs repair = 1, non functional = 2
                # ArÄ±za oranÄ± = non functional sayÄ±sÄ± / toplam
                failure_rate = train_new.groupby(col)['status_group'].apply(
                    lambda x: (x == 2).sum() / len(x)
                ).to_dict()
                
                train_new[f'{col}_failure_rate'] = train_new[col].map(failure_rate)
                test_new[f'{col}_failure_rate'] = test_new[col].map(failure_rate)
                print(f"âœ“ '{col}_failure_rate' oluÅŸturuldu")
                print(f"  Ortalama arÄ±za oranÄ±: {train_new[f'{col}_failure_rate'].mean():.2%}")
    
    print(f"\n{'='*60}")
    print("Toplama BazlÄ± Ã–zellikler OluÅŸturuldu!")
    print(f"{'='*60}")
    
    return train_new, test_new

# Gruplama yapÄ±lacak sÃ¼tunlar
group_columns = ['region', 'basin', 'installer', 'scheme_management', 'extraction_type']

# Ã–zellikleri oluÅŸtur
train_final, test_final = create_aggregated_features(
    train_with_geo, 
    test_with_geo, 
    group_columns
)



# %%

## 6. Model GeliÅŸtirme

### 6.1 Veri Setini BÃ¶lme

from sklearn.model_selection import train_test_split

def prepare_modeling_data(df, target_col='status_group', test_size=0.2, random_state=42):
    """
    Veriyi X (features) ve y (target) olarak ayÄ±rÄ±r ve train-validation split yapar
    
    Parameters:
    -----------
    df : DataFrame
        Ã–zellik mÃ¼hendisliÄŸi yapÄ±lmÄ±ÅŸ veri seti
    target_col : str
        Hedef deÄŸiÅŸken sÃ¼tunu
    test_size : float
        Validation set oranÄ±
    random_state : int
        Reproducibility iÃ§in seed
    
    Returns:
    --------
    tuple
        (X_train, X_val, y_train, y_val, feature_names)
    """
    # Kategorik yaÅŸ gibi object tipli yeni sÃ¼tunlarÄ± encode et
    df_model = df.copy()
    
    # Object ve category tipli sÃ¼tunlarÄ± encode et (Ã¶nceden yapÄ±lmamÄ±ÅŸsa)
    from sklearn.preprocessing import LabelEncoder
    
    for col in df_model.select_dtypes(include=['object', 'category']).columns:
        if col != target_col and col != 'id':
            le = LabelEncoder()
            df_model[col] = le.fit_transform(df_model[col].astype(str))
    
    # ID ve date_recorded'Ä± Ã§Ä±kar
    drop_cols = ['id']
    if 'date_recorded' in df_model.columns:
        drop_cols.append('date_recorded')
    
    # X ve y'yi ayÄ±r
    X = df_model.drop(columns=drop_cols + [target_col])
    y = df_model[target_col]
    
    # Feature isimlerini sakla
    feature_names = X.columns.tolist()
    
    # Train-Validation split
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, 
        test_size=test_size, 
        random_state=random_state,
        stratify=y  # SÄ±nÄ±f dengesini koru
    )
    
    print(f"{'='*60}")
    print("Veri Seti HazÄ±rlandÄ±!")
    print(f"{'='*60}")
    print(f"Toplam Ã¶zellik sayÄ±sÄ±: {len(feature_names)}")
    print(f"EÄŸitim seti boyutu: {X_train.shape}")
    print(f"Validation seti boyutu: {X_val.shape}")
    print(f"\nSÄ±nÄ±f daÄŸÄ±lÄ±mÄ± (Train):")
    print(y_train.value_counts())
    print(f"\nSÄ±nÄ±f daÄŸÄ±lÄ±mÄ± (Validation):")
    print(y_val.value_counts())
    
    return X_train, X_val, y_train, y_val, feature_names

# Veriyi hazÄ±rla
X_train, X_val, y_train, y_val, features = prepare_modeling_data(train_final)


### 6.2 Baseline Model (Random Forest)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import time

def train_random_forest(X_train, y_train, X_val, y_val):
    """
    Random Forest modeli eÄŸitir ve deÄŸerlendirir
    
    Parameters:
    -----------
    X_train, y_train : array-like
        EÄŸitim verisi
    X_val, y_val : array-like
        Validation verisi
    
    Returns:
    --------
    RandomForestClassifier
        EÄŸitilmiÅŸ model
    """
    print(f"{'='*60}")
    print("Random Forest Modeli EÄŸitiliyor...")
    print(f"{'='*60}\n")
    
    # Model parametreleri
    rf_params = {
        'n_estimators': 100,        # AÄŸaÃ§ sayÄ±sÄ±
        'max_depth': 20,             # Maksimum derinlik
        'min_samples_split': 10,     # Split iÃ§in minimum Ã¶rnek
        'min_samples_leaf': 4,       # Yaprakta minimum Ã¶rnek
        'random_state': 42,
        'n_jobs': -1,                # TÃ¼m CPU core'larÄ± kullan
        'class_weight': 'balanced'   # Dengesiz sÄ±nÄ±flarÄ± dengele
    }
    
    # Modeli oluÅŸtur
    rf_model = RandomForestClassifier(**rf_params)
    
    # EÄŸitim sÃ¼resi Ã¶lÃ§
    start_time = time.time()
    
    # Modeli eÄŸit
    rf_model.fit(X_train, y_train)
    
    training_time = time.time() - start_time
    
    print(f"âœ“ Model eÄŸitildi! SÃ¼re: {training_time:.2f} saniye\n")
    
    # Tahminler
    y_train_pred = rf_model.predict(X_train)
    y_val_pred = rf_model.predict(X_val)
    
    # Performans metrikleri
    train_accuracy = accuracy_score(y_train, y_train_pred)
    val_accuracy = accuracy_score(y_val, y_val_pred)
    
    print(f"{'â”€'*60}")
    print("Model PerformansÄ±:")
    print(f"{'â”€'*60}")
    print(f"EÄŸitim Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
    print(f"Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)")
    # Overfitting kontrolÃ¼
    overfit_diff = train_accuracy - val_accuracy
    if overfit_diff > 0.05:
        print(f"\nâš ï¸  Overfitting tespit edildi! Fark: {overfit_diff:.4f}")
    else:
        print(f"\nâœ“ Overfitting yok. Fark: {overfit_diff:.4f}")
    
    # DetaylÄ± sÄ±nÄ±flandÄ±rma raporu
    print(f"\n{'â”€'*60}")
    print("SÄ±nÄ±flandÄ±rma Raporu (Validation Set):")
    print(f"{'â”€'*60}")
    print(classification_report(y_val, y_val_pred, 
                                target_names=['Functional', 'Needs Repair', 'Non Functional']))
    
    # Confusion Matrix
    print(f"{'â”€'*60}")
    print("Confusion Matrix:")
    print(f"{'â”€'*60}")
    cm = confusion_matrix(y_val, y_val_pred)
    print(cm)
    
    # Confusion matrix gÃ¶rselleÅŸtirme
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Functional', 'Needs Repair', 'Non Functional'],
                yticklabels=['Functional', 'Needs Repair', 'Non Functional'])
    plt.title('Confusion Matrix - Random Forest')
    plt.ylabel('GerÃ§ek DeÄŸer')
    plt.xlabel('Tahmin')
    plt.tight_layout()
    plt.savefig('confusion_matrix_rf.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return rf_model
# Random Forest modelini eÄŸit
rf_model = train_random_forest(X_train, y_train, X_val, y_val)

# %%

    
    

# %%
### 6.3 Ã–zellik Ã–nem Analizi

def analyze_feature_importance(model, feature_names, top_n=20):
    """
    Model'in Ã¶zellik Ã¶nem skorlarÄ±nÄ± analiz eder ve gÃ¶rselleÅŸtirir
    
    Parameters:
    -----------
    model : sklearn model
        EÄŸitilmiÅŸ model (feature_importances_ attribute'u olmalÄ±)
    feature_names : list
        Ã–zellik isimleri
    top_n : int
        GÃ¶sterilecek en Ã¶nemli Ã¶zellik sayÄ±sÄ±
    """
    # Ã–zellik Ã¶nem skorlarÄ±
    importances = model.feature_importances_
    
    # DataFrame oluÅŸtur
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    print(f"{'='*60}")
    print(f"En Ã–nemli {top_n} Ã–zellik:")
    print(f"{'='*60}\n")
    
    for idx, row in feature_importance_df.head(top_n).iterrows():
        print(f"{row['feature']:30s} : {row['importance']:.6f}")
    
    # GÃ¶rselleÅŸtirme
    plt.figure(figsize=(10, 8))
    
    top_features = feature_importance_df.head(top_n)
    
    plt.barh(range(len(top_features)), top_features['importance'].values, 
             color='steelblue')
    plt.yticks(range(len(top_features)), top_features['feature'].values)
    plt.xlabel('Ã–nem Skoru', fontsize=12)
    plt.title(f'En Ã–nemli {top_n} Ã–zellik', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()  # En Ã¶nemli Ã¼stte
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return feature_importance_df

# Ã–zellik Ã¶nemini analiz et
feature_importance = analyze_feature_importance(rf_model, features, top_n=20)

# %%

### 6.4 Gradient Boosting Modelleri (XGBoost & LightGBM)

import xgboost as xgb
import lightgbm as lgb

def train_xgboost(X_train, y_train, X_val, y_val):
    """
    XGBoost modeli eÄŸitir
    
    Returns:
    --------
    xgb.XGBClassifier
        EÄŸitilmiÅŸ model
    """
    print(f"{'='*60}")
    print("XGBoost Modeli EÄŸitiliyor...")
    print(f"{'='*60}\n")
    
    # SÄ±nÄ±f aÄŸÄ±rlÄ±klarÄ±nÄ± hesapla
    from sklearn.utils.class_weight import compute_class_weight
    
    class_weights = compute_class_weight(
        'balanced', 
        classes=np.unique(y_train), 
        y=y_train
    )
    
    # XGBoost parametreleri
    xgb_params = {
        'n_estimators': 200,
        'max_depth': 8,
        'learning_rate': 0.1,
        'subsample': 0.8,           # Her aÄŸaÃ§ iÃ§in rastgele %80 veri kullan
        'colsample_bytree': 0.8,    # Her aÄŸaÃ§ iÃ§in rastgele %80 Ã¶zellik kullan
        'objective': 'multi:softmax',
        'num_class': 3,
        'random_state': 42,
        'n_jobs': -1,
        'eval_metric': 'mlogloss'
    }
    
    # Modeli oluÅŸtur
    xgb_model = xgb.XGBClassifier(**xgb_params)
    
    # EÄŸitim sÄ±rasÄ±nda validation setini izle
    eval_set = [(X_train, y_train), (X_val, y_val)]
    
    start_time = time.time()
    
    xgb_model.fit(
        X_train, y_train,
        eval_set=eval_set,
        verbose=50  # Her 50 iterasyonda bir log bas
    )
    
    training_time = time.time() - start_time
    
    print(f"\nâœ“ Model eÄŸitildi! SÃ¼re: {training_time:.2f} saniye\n")
    
    # Performans deÄŸerlendirme
    y_val_pred = xgb_model.predict(X_val)
    val_accuracy = accuracy_score(y_val, y_val_pred)
    
    print(f"Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)")
    
    return xgb_model

# %%
def train_lightgbm(X_train, y_train, X_val, y_val):
    """
    LightGBM modeli eÄŸitir
    
    Returns:
    --------
    lgb.LGBMClassifier
        EÄŸitilmiÅŸ model
    """
    print(f"{'='*60}")
    print("LightGBM Modeli EÄŸitiliyor...")
    print(f"{'='*60}\n")
    
    # LightGBM parametreleri
    lgb_params = {
        'n_estimators': 200,
        'max_depth': 8,
        'learning_rate': 0.1,
        'num_leaves': 31,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'objective': 'multiclass',
        'num_class': 3,
        'random_state': 42,
        'n_jobs': -1,
        'class_weight': 'balanced'
    }
    
    # Modeli oluÅŸtur
    lgb_model = lgb.LGBMClassifier(**lgb_params)
    
    start_time = time.time()
    
    lgb_model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        eval_metric='multi_logloss',
        callbacks=[lgb.log_evaluation(50)]  # Her 50 iterasyonda log
    )
    
    training_time = time.time() - start_time
    
    print(f"\nâœ“ Model eÄŸitildi! SÃ¼re: {training_time:.2f} saniye\n")
    
    # Performans deÄŸerlendirme
    y_val_pred = lgb_model.predict(X_val)
    val_accuracy = accuracy_score(y_val, y_val_pred)
    
    print(f"Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)")
    
    return lgb_model

# %%
# Her iki modeli de eÄŸit
xgb_model = train_xgboost(X_train, y_train, X_val, y_val)
lgb_model = train_lightgbm(X_train, y_train, X_val, y_val)

# %%


### 6.5 Model KarÅŸÄ±laÅŸtÄ±rma ve En Ä°yi Model SeÃ§imi

def compare_models(models_dict, X_val, y_val):
    """
    Birden fazla modeli karÅŸÄ±laÅŸtÄ±rÄ±r
    
    Parameters:
    -----------
    models_dict : dict
        Model isimleri ve model objeleri
    X_val, y_val : array-like
        Validation verisi
    
    Returns:
    --------
    DataFrame
        Model performans karÅŸÄ±laÅŸtÄ±rmasÄ±
    """
    from sklearn.metrics import f1_score, precision_score, recall_score
    
    results = []
    
    print(f"{'='*60}")
    print("Model KarÅŸÄ±laÅŸtÄ±rmasÄ±")
    print(f"{'='*60}\n")
    
    for name, model in models_dict.items():
        # Tahminler
        y_pred = model.predict(X_val)
        
        # Metrikler
        accuracy = accuracy_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred, average='weighted')
        precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_val, y_pred, average='weighted')
        
        results.append({
            'Model': name,
            'Accuracy': accuracy,
            'F1-Score': f1,
            'Precision': precision,
            'Recall': recall
        })
        
        print(f"{name}:")
        print(f"  Accuracy : {accuracy:.4f}")
        print(f"  F1-Score : {f1:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall   : {recall:.4f}\n")
    
    # DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
    results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)
    
    # GÃ¶rselleÅŸtirme
    fig, ax = plt.subplots(figsize=(10, 6))
    
    x = np.arange(len(results_df))
    width = 0.2
    
    metrics = ['Accuracy', 'F1-Score', 'Precision', 'Recall']
    colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']
    
    for i, metric in enumerate(metrics):
        ax.bar(x + i*width, results_df[metric], width, 
               label=metric, color=colors[i])
    
    ax.set_xlabel('Model', fontsize=12)
    ax.set_ylabel('Skor', fontsize=12)
    ax.set_title('Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=14, fontweight='bold')
    ax.set_xticks(x + width * 1.5)
    ax.set_xticklabels(results_df['Model'])
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # En iyi modeli seÃ§
    best_model_name = results_df.iloc[0]['Model']
    best_accuracy = results_df.iloc[0]['Accuracy']
    
    print(f"\n{'='*60}")
    print(f"ğŸ† En Ä°yi Model: {best_model_name}")
    print(f"   Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)")
    print(f"{'='*60}")
    
    return results_df, models_dict[best_model_name]

# Modelleri karÅŸÄ±laÅŸtÄ±r
models = {
    'Random Forest': rf_model,
    'XGBoost': xgb_model,
    'LightGBM': lgb_model
}

comparison_results, best_model = compare_models(models, X_val, y_val)



## 7. Model Optimizasyonu (Hyperparameter Tuning)

### 7.1 Grid Search ile Parametre Optimizasyonu

from sklearn.model_selection import GridSearchCV

def optimize_model_gridsearch(X_train, y_train, model_type='xgboost'):
    """
    Grid Search ile hiperparametre optimizasyonu yapar
    
    Parameters:
    -----------
    X_train, y_train : array-like
        EÄŸitim verisi
    model_type : str
        'xgboost', 'lightgbm', veya 'random_forest'
    
    Returns:
    --------
    model
        Optimize edilmiÅŸ en iyi model
    """
    print(f"{'='*60}")
    print(f"{model_type.upper()} - Grid Search BaÅŸlatÄ±lÄ±yor...")
    print(f"{'='*60}\n")
    
    if model_type == 'xgboost':
        # XGBoost iÃ§in parametre grid'i
        model = xgb.XGBClassifier(
            objective='multi:softmax',
            num_class=3,
            random_state=42,
            n_jobs=-1
        )
        
        param_grid = {
            'n_estimators': [100, 200],
            'max_depth': [6, 8, 10],
            'learning_rate': [0.05, 0.1, 0.2],
            'subsample': [0.8, 0.9],
            'colsample_bytree': [0.8, 0.9]
        }
    
    elif model_type == 'lightgbm':
        # LightGBM iÃ§in parametre grid'i
        model = lgb.LGBMClassifier(
            objective='multiclass',
            num_class=3,
            random_state=42,
            n_jobs=-1
        )
        
        param_grid = {
            'n_estimators': [100, 200],
            'max_depth': [6, 8, 10],
            'learning_rate': [0.05, 0.1, 0.2],
            'num_leaves': [31, 50],
            'subsample': [0.8, 0.9]
        }
    
    else:  # random_forest
        model = RandomForestClassifier(
            random_state=42,
            n_jobs=-1
        )
        
        param_grid = {
            'n_estimators': [100, 200],
            'max_depth': [15, 20, 25],
            'min_samples_split': [5, 10],
            'min_samples_leaf': [2, 4]
        }
    
    # Grid Search
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=3,                    # 3-fold cross validation
        scoring='accuracy',
        verbose=2,
        n_jobs=-1
    )
    
    print("Grid Search Ã§alÄ±ÅŸÄ±yor... (Bu biraz zaman alabilir)\n")
    
    start_time = time.time()
    grid_search.fit(X_train, y_train)
    search_time = time.time() - start_time
    
    print(f"\nâœ“ Grid Search tamamlandÄ±! SÃ¼re: {search_time/60:.2f} dakika\n")
    
    # En iyi parametreler
    print(f"{'â”€'*60}")
    print("En Ä°yi Parametreler:")
    print(f"{'â”€'*60}")
    for param, value in grid_search.best_params_.items():
        print(f"  {param}: {value}")
    
    print(f"\nEn Ä°yi Cross-Validation Accuracy: {grid_search.best_score_:.4f}")
    
    return grid_search.best_estimator_

# Ã–rnek: XGBoost iÃ§in optimizasyon (opsiyonel - uzun sÃ¼rer)
# optimized_xgb = optimize_model_gridsearch(X_train, y_train, 'xgboost')


## 8. Test Seti Tahminleri ve Submission

### 8.1 Test Setinde Tahmin Yapma

def prepare_test_data(test_df, feature_names):
    """
    Test verisini modele uygun formata getirir
    
    Parameters:
    -----------
    test_df : DataFrame
        Test veri seti (Ã¶zellik mÃ¼hendisliÄŸi yapÄ±lmÄ±ÅŸ)
    feature_names : list
        Modelin eÄŸitildiÄŸi Ã¶zellik isimleri
    
    Returns:
    --------
    tuple
        (test_ids, X_test)
    """
    # ID'leri sakla
    test_ids = test_df['id'].copy()
    
    # Kategorik sÃ¼tunlarÄ± encode et
    test_prepared = test_df.copy()
    
    from sklearn.preprocessing import LabelEncoder
    
    for col in test_prepared.select_dtypes(include=['object']).columns:
        if col != 'id':
            le = LabelEncoder()
            test_prepared[col] = le.fit_transform(test_prepared[col].astype(str))
    
    # ID ve date_recorded'Ä± Ã§Ä±kar
    drop_cols = ['id']
    if 'date_recorded' in test_prepared.columns:
        drop_cols.append('date_recorded')
    
    X_test = test_prepared.drop(columns=drop_cols)
    
    # Sadece eÄŸitimde kullanÄ±lan Ã¶zellikleri al
    # (Yeni Ã¶zellikler varsa Ã§Ä±kar, eksik olanlarÄ± ekle)
    missing_features = set(feature_names) - set(X_test.columns)
    extra_features = set(X_test.columns) - set(feature_names)
    
    if missing_features:
        print(f"âš ï¸  Eksik Ã¶zellikler ekleniyor: {missing_features}")
        for feat in missing_features:
            X_test[feat] = 0
    
    if extra_features:
        print(f"âš ï¸  Fazla Ã¶zellikler Ã§Ä±karÄ±lÄ±yor: {extra_features}")
        X_test = X_test.drop(columns=list(extra_features))
    
    # SÃ¼tun sÄ±rasÄ±nÄ± eÄŸitim setiyle aynÄ± yap
    X_test = X_test[feature_names]
    
    print(f"\n{'='*60}")
    print("Test Verisi HazÄ±r!")
    print(f"{'='*60}")
    print(f"Test set boyutu: {X_test.shape}")
    print(f"Ã–zellik sayÄ±sÄ±: {X_test.shape[1]}")
    
    return test_ids, X_test

# Test verisini hazÄ±rla
test_ids, X_test = prepare_test_data(test_final, features)


# %%

### 8.2 Tahmin ve Submission DosyasÄ± OluÅŸturma

def create_submission(model, test_ids, X_test, encoders, filename='submission.csv'):
    """
    Test seti tahminlerini yapar ve submission dosyasÄ± oluÅŸturur
    
    Parameters:
    -----------
    model : sklearn model
        EÄŸitilmiÅŸ model
    test_ids : Series
        Test seti ID'leri
    X_test : DataFrame
        Test Ã¶zellikleri
    encoders : dict
        Label encoders (target'Ä± decode etmek iÃ§in)
    filename : str
        Ã‡Ä±ktÄ± dosya adÄ±
    
    Returns:
    --------
    DataFrame
        Submission dosyasÄ±
    """
    print(f"{'='*60}")
    print("Test Seti Tahminleri YapÄ±lÄ±yor...")
    print(f"{'='*60}\n")
    
    # Tahminler
    predictions = model.predict(X_test)
    
    # Encode edilmiÅŸ deÄŸerleri orijinal sÄ±nÄ±f isimlerine Ã§evir
    if 'status_group' in encoders:
        target_encoder = encoders['status_group']
        predictions_decoded = target_encoder.inverse_transform(predictions)
    else:
        # Manuel decode (eÄŸer encoder yoksa)
        class_mapping = {0: 'functional', 1: 'functional needs repair', 2: 'non functional'}
        predictions_decoded = [class_mapping[p] for p in predictions]
    
    # Submission DataFrame
    submission_df = pd.DataFrame({
        'id': test_ids,
        'status_group': predictions_decoded
    })
    
    # CSV'ye kaydet
    submission_df.to_csv(filename, index=False)
    
    print(f"âœ“ Submission dosyasÄ± oluÅŸturuldu: {filename}")
    print(f"  Toplam tahmin sayÄ±sÄ±: {len(submission_df):,}")
    print(f"\nTahmin DaÄŸÄ±lÄ±mÄ±:")
    print(submission_df['status_group'].value_counts())
    print(f"\nTahmin DaÄŸÄ±lÄ±mÄ± (%):")
    print(submission_df['status_group'].value_counts(normalize=True) * 100)
    
    # Ä°lk 10 tahmini gÃ¶ster
    print(f"\n{'â”€'*60}")
    print("Ä°lk 10 Tahmin:")
    print(f"{'â”€'*60}")
    print(submission_df.head(10))
    
    return submission_df

# Submission oluÅŸtur
submission = create_submission(
    model=best_model,
    test_ids=test_ids,
    X_test=X_test,
    encoders=encoders,
    filename='submission.csv'
)



# %%


## 9. Model Kaydetme ve DaÄŸÄ±tÄ±m

### 9.1 Model ve Preprocessor'larÄ± Kaydetme

import pickle
import joblib

def save_model_and_artifacts(model, encoders, scaler, feature_names, 
                             model_name='best_model'):
    """
    Modeli ve tÃ¼m preprocessing araÃ§larÄ±nÄ± kaydeder
    
    Parameters:
    -----------
    model : sklearn model
        EÄŸitilmiÅŸ model
    encoders : dict
        Label encoders
    scaler : StandardScaler
        Feature scaler
    feature_names : list
        Ã–zellik isimleri
    model_name : str
        Model dosya adÄ±
    """
    import os
    
    # models klasÃ¶rÃ¼nÃ¼ oluÅŸtur
    os.makedirs('models', exist_ok=True)
    
    # Model
    model_path = f'models/{model_name}.pkl'
    joblib.dump(model, model_path)
    print(f"âœ“ Model kaydedildi: {model_path}")
    
    # Encoders
    encoders_path = f'models/{model_name}_encoders.pkl'
    joblib.dump(encoders, encoders_path)
    print(f"âœ“ Encoders kaydedildi: {encoders_path}")
    
    # Scaler
    scaler_path = f'models/{model_name}_scaler.pkl'
    joblib.dump(scaler, scaler_path)
    print(f"âœ“ Scaler kaydedildi: {scaler_path}")
    
    # Feature names
    features_path = f'models/{model_name}_features.pkl'
    joblib.dump(feature_names, features_path)
    print(f"âœ“ Feature names kaydedildi: {features_path}")
    
    # Metadata (model bilgileri)
    metadata = {
        'model_type': type(model).__name__,
        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'num_features': len(feature_names),
        'feature_names': feature_names
    }
    
    metadata_path = f'models/{model_name}_metadata.pkl'
    joblib.dump(metadata, metadata_path)
    print(f"âœ“ Metadata kaydedildi: {metadata_path}")
    
    print(f"\n{'='*60}")
    print("TÃ¼m Artifactler Kaydedildi!")
    print(f"{'='*60}")

# Model ve artifactleri kaydet
save_model_and_artifacts(
    model=best_model,
    encoders=encoders,
    scaler=scaler,
    feature_names=features,
    model_name='water_pump_model_v1'
)


# %% [markdown]
# 
# ### 9.2 Modeli YÃ¼kleme ve Tahmin Yapma
# 
# def load_model_and_predict(new_data_path, model_name='water_pump_model_v1'):
#     """
#     KaydedilmiÅŸ modeli yÃ¼kler ve yeni veriler Ã¼zerinde tahmin yapar
#     
#     Parameters:
#     -----------
#     new_data_path : str
#         Yeni veri dosyasÄ± yolu
#     model_name : str
#         YÃ¼klenecek model adÄ±
#     
#     Returns:
#     --------
#     DataFrame
#         Tahminler
#     """
#     print(f"{'='*60}")
#     print(f"Model YÃ¼kleniyor: {model_name}")
#     print(f"{'='*60}\n")
#     
#     # Artifactleri yÃ¼kle
#     model = joblib.load(f'models/{model_name}.pkl')
#     encoders = joblib.load(f'models/{model_name}_encoders.pkl')
#     scaler = joblib.load(f'models/{model_name}_scaler.pkl')
#     feature_names = joblib.load(f'models/{model_name}_features.pkl')
#     metadata = joblib.load(f'models/{model_name}_metadata.pkl')
#     
#     print(f"âœ“ Model yÃ¼klendi: {metadata['model_type']}")
#     print(f"  EÄŸitim tarihi: {metadata['training_date']}")
#     print(f"  Ã–zellik sayÄ±sÄ±: {metadata['num_features']}\n")
#     
#     # Yeni veriyi yÃ¼kle
#     new_data = pd.read_csv(new_data_path)
#     print(f"âœ“ Yeni veri yÃ¼klendi: {new_data.shape}\n")
#     
#     # Preprocessing pipeline'Ä± uygula
#     # (Burada tÃ¼m preprocessing adÄ±mlarÄ± tekrar uygulanmalÄ±)
#     # 1. Eksik deÄŸer iÅŸleme
#     # 2. Encoding
#     # 3. Feature engineering
#     # 4. Scaling
#     
#     print("Preprocessing uygulanÄ±yor...")
#     
#     # ... (tÃ¼m preprocessing fonksiyonlarÄ± burada Ã§aÄŸrÄ±lÄ±r)
#     
#     # Tahmin
#     predictions = model.predict(new_data[feature_names])
#     
#     # Decode
#     if 'status_group' in encoders:
#         predictions_decoded = encoders['status_group'].inverse_transform(predictions)
#     
#     # SonuÃ§ DataFrame
#     result_df = pd.DataFrame({
#         'id': new_data['id'],
#         'predicted_status': predictions_decoded
#     })
#     
#     print(f"\n{'='*60}")
#     print("Tahminler TamamlandÄ±!")
#     print(f"{'='*60}")
#     
#     return result_df
# 
# # Ã–rnek kullanÄ±m (yeni veri geldiÄŸinde)
# # new_predictions = load_model_and_predict('new_data.csv')
# 
# 

# %%

## 10. Proje SonuÃ§larÄ± ve Ä°ÅŸ DeÄŸeri

### 10.1 Performans Ã–zeti

def generate_project_report(comparison_results, best_model, feature_importance):
    """
    Proje sonuÃ§larÄ±nÄ± Ã¶zetleyen bir rapor oluÅŸturur
    
    Parameters:
    -----------
    comparison_results : DataFrame
        Model karÅŸÄ±laÅŸtÄ±rma sonuÃ§larÄ±
    best_model : sklearn model
        En iyi model
    feature_importance : DataFrame
        Ã–zellik Ã¶nem skorlarÄ±
    """
    print(f"\n{'='*80}")
    print(" " * 20 + "PROJE SONUÃ‡ RAPORU")
    print(f"{'='*80}\n")
    
    print("ğŸ“Š MODEL PERFORMANSI")
    print(f"{'â”€'*80}")
    print(comparison_results.to_string(index=False))
    
    print(f"\n\nğŸ† EN Ä°YÄ° MODEL")
    print(f"{'â”€'*80}")
    best_row = comparison_results.iloc[0]
    print(f"Model AdÄ±    : {best_row['Model']}")
    print(f"Accuracy     : {best_row['Accuracy']:.4f} ({best_row['Accuracy']*100:.2f}%)")
    print(f"F1-Score     : {best_row['F1-Score']:.4f}")
    print(f"Precision    : {best_row['Precision']:.4f}")
    print(f"Recall       : {best_row['Recall']:.4f}")
    
    print(f"\n\nğŸ”‘ EN Ã–NEMLÄ° 10 Ã–ZELLÄ°K")
    print(f"{'â”€'*80}")
    top_10_features = feature_importance.head(10)
    for idx, row in top_10_features.iterrows():
        bar_length = int(row['importance'] * 100)
        bar = 'â–ˆ' * bar_length
        print(f"{row['feature']:30s} : {bar} {row['importance']:.4f}")
    
    print(f"\n\nğŸ’¡ Ä°Å DEÄERÄ° VE Ã–NERÄ°LER")
    print(f"{'â”€'*80}")
    print("""

# %%


# %%



